# machine-tasks
Datasets for compositional learning

This repository contains several datasets used to evaluate to what extent a system has learned a compositional or systematic solution. Currently, it contains four tasks.

## SCAN

A dataset proposed in [[1]](https://arxiv.org/abs/1711.00350) of mapping simple input instructions to an output sequence, designed to evaluate to what extent a network learns a systematic solution.

## Lookup-tables

A dataset proposed in [[2]](https://arxiv.org/abs/1802.06467) to evaluate compositional learning.

## Symbol-rewriting

A dataset proposed in [[3]](https://arxiv.org/abs/1805.01445) to evaluate a model's ability to generalise on a simple symbol rewriting task.

## Number Prediction

A dataset proposed in [[4]](https://arxiv.org/pdf/1611.01368) to evaluate a model's ability to learn hierarchical structures in natural language.

## References
\[1\] Brenden M. Lake and Marco Baroni. Generalization without systematicity: On the compositional skills of sequence-to-sequence recurrent networks, 2017. <br />
\[2\] Adam Liška, Germán Kruszewski, and Marco Baroni. Memorize or generalize? searching for a
compositional rnn in a haystack, 2018. <br />
\[3\] Noah Weber, Leena Shekhar, and Niranjan Balasubramanian. The fine line between linguistic
generalization and failure in seq2seq-attention models. In Workshop on new forms of generalization
in deep learning and natural language processing, NAACL’18, 2018. <br />
\[4\] Tal Linzen, Emmanuel Dupoux, and Yoav Goldberg. 2016. Assessing the ability of lstms to learn
syntax sensitive dependencies. Transactions of the Association for Computational Linguistics, 4:521–535.
